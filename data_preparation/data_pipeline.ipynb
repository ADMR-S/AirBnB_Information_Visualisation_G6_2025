{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d32c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stargazer/dev/AirBnB_Information_Visualisation_G6_2025/data_preparation/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70e53c",
   "metadata": {},
   "source": [
    "#### 1. Start: download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbe0cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Downloading dataset...\n",
      "‚úì Dataset downloaded to: /home/stargazer/.cache/kagglehub/datasets/kritikseth/us-airbnb-open-data/versions/2\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"kritikseth/us-airbnb-open-data\")\n",
    "print(f\"‚úì Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe26cf",
   "metadata": {},
   "source": [
    "#### 2. Start: listing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fd6b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Scanning dataset folder for CSV files...\n",
      "‚úì CSV files found: ['AB_US_2020.csv', 'AB_US_2023.csv']\n",
      "\n",
      "\n",
      "‚ö°Ô∏é Loading AB_US_2020.csv ...\n",
      "‚úì AB_US_2020.csv loaded ‚Üí shape: (226030, 17)\n",
      "\n",
      "\n",
      "‚ö°Ô∏é Loading AB_US_2023.csv ...\n",
      "‚úì AB_US_2023.csv loaded ‚Üí shape: (232147, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Scanning dataset folder for CSV files...\")\n",
    "csv_files = [f for f in os.listdir(path) if f.lower().endswith(\".csv\")]\n",
    "print(f\"‚úì CSV files found: {csv_files}\")\n",
    "\n",
    "dataframes = {}\n",
    "for csv in csv_files:\n",
    "    print(\"\\n\")\n",
    "    print(f\"‚ö°Ô∏é Loading {csv} ...\")\n",
    "    csv_path = os.path.join(path, csv)\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    dataframes[csv] = df\n",
    "    print(f\"‚úì {csv} loaded ‚Üí shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80b49b",
   "metadata": {},
   "source": [
    "#### 3. Start: check for 2020 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16e1e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Checking for specific datasets (2020 & 2023)...\n",
      "‚úì Found 2020 dataset: True\n",
      "‚úì Found 2023 dataset: True\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Checking for specific datasets (2020 & 2023)...\")\n",
    "df_2020 = dataframes.get(\"AB_US_2020.csv\")\n",
    "df_2023 = dataframes.get(\"AB_US_2023.csv\")\n",
    "print(f\"‚úì Found 2020 dataset: {df_2020 is not None}\")\n",
    "print(f\"‚úì Found 2023 dataset: {df_2023 is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3c329",
   "metadata": {},
   "source": [
    "#### 4. Clean up: drop unwanted columns if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e68597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Cleaning up datasets...\n",
      ". Initial 2020 dataset shape: (226030, 17)\n",
      ". Initial 2023 dataset shape: (232147, 18)\n",
      "‚úì 2020 dataset shape after cleanup: (226030, 16)\n",
      "‚úì 2023 dataset shape after cleanup: (232147, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Cleaning up datasets...\")\n",
    "print(f\". Initial 2020 dataset shape: {df_2020.shape}\")\n",
    "print(f\". Initial 2023 dataset shape: {df_2023.shape}\")\n",
    "\n",
    "to_drop = [\"neighbourhood_group\", \"number_of_reviews_ltm\"]\n",
    "\n",
    "for col in to_drop:\n",
    "    if col in df_2020.columns:\n",
    "        df_2020 = df_2020.drop(columns=col)\n",
    "    if col in df_2023.columns:\n",
    "        df_2023 = df_2023.drop(columns=col) \n",
    "\n",
    "\n",
    "\n",
    "# Make sure the columns match\n",
    "assert list(df_2020.columns) == list(df_2023.columns), (\n",
    "    \"Columns are not the same after cleanup!\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì 2020 dataset shape after cleanup: {df_2020.shape}\")\n",
    "print(f\"‚úì 2023 dataset shape after cleanup: {df_2023.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b14882",
   "metadata": {},
   "source": [
    "#### 5. Add 'year' column and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0490dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Merged dataset shape: (458177, 17)\n",
      "‚úì Columns: ['id', 'name', 'host_id', 'host_name', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'city', 'year']\n"
     ]
    }
   ],
   "source": [
    "df_2020[\"year\"] = \"2020\"\n",
    "df_2023[\"year\"] = \"2023\"\n",
    "\n",
    "df = pd.concat([df_2020, df_2023], ignore_index=True)\n",
    "\n",
    "print(f\"‚úì Merged dataset shape: {df.shape}\")\n",
    "print(f\"‚úì Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c4d8a",
   "metadata": {},
   "source": [
    "#### 6. Map each city to its US state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db117651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Correcting city typos...\n",
      "‚ö°Ô∏é Mapping cities to states...\n",
      "‚úì 'state' column added with 19 unique values.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Correcting city typos...\")\n",
    "df[\"city\"] = df[\"city\"].replace({\n",
    "    \"San Clara Country\": \"Santa Clara County\"\n",
    "})\n",
    "\n",
    "city_to_state = {\n",
    "    \"New York City\": \"NY\",\n",
    "    \"Los Angeles\": \"CA\",\n",
    "    \"Broward County\": \"FL\",\n",
    "    \"San Diego\": \"CA\",\n",
    "    \"Austin\": \"TX\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Clark County\": \"NV\",\n",
    "    \"Nashville\": \"TN\",\n",
    "    \"Chicago\": \"IL\",\n",
    "    \"San Francisco\": \"CA\",\n",
    "    \"Washington D.C.\": \"DC\",\n",
    "    \"New Orleans\": \"LA\",\n",
    "    \"Seattle\": \"WA\",\n",
    "    \"Twin Cities MSA\": \"MN\",\n",
    "    \"Denver\": \"CO\",\n",
    "    \"Portland\": \"OR\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"Boston\": \"MA\",\n",
    "    \"Santa Clara County\": \"CA\",\n",
    "    \"San Mateo County\": \"CA\",\n",
    "    \"Oakland\": \"CA\",\n",
    "    \"Asheville\": \"NC\",\n",
    "    \"Jersey City\": \"NJ\",\n",
    "    \"Columbus\": \"OH\",\n",
    "    \"Santa Cruz County\": \"CA\",\n",
    "    \"Cambridge\": \"MA\",\n",
    "    \"Salem\": \"MA\",\n",
    "    \"Pacific Grove\": \"CA\"\n",
    "}\n",
    "\n",
    "print(\"‚ö°Ô∏é Mapping cities to states...\")\n",
    "df[\"state\"] = df[\"city\"].map(city_to_state)\n",
    "print(f\"‚úì 'state' column added with {df['state'].nunique()} unique values.\")\n",
    "\n",
    "# Check if any city didn't get mapped\n",
    "missing = df[df[\"state\"].isna()][\"city\"].unique()\n",
    "if len(missing):\n",
    "    print(f\"! Some cities are missing state mappings: {missing}\")\n",
    "    raise ValueError(\"Some cities are missing state mappings!\" + str(missing))\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73edc13",
   "metadata": {},
   "source": [
    "#### 7. Remove semantic duplicates (same host/listing details but different id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330569c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Removing semantic duplicates (same host/listing details but different id)...\n",
      "‚úì Removed 971 duplicates. New shape: (457206, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Removing semantic duplicates (same host/listing details but different id)...\")\n",
    "before = df.shape\n",
    "df = df.drop_duplicates(\n",
    "    subset=[\"host_id\", \"name\", \"latitude\", \"longitude\", \"room_type\", \"price\",\n",
    "            \"minimum_nights\", \"availability_365\", \"city\", \"year\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "print(f\"‚úì Removed {before[0] - df.shape[0]} duplicates. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccb2b0",
   "metadata": {},
   "source": [
    "#### 8. Remove duplicate of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c639f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Initial dataset shape before collapsing: (457206, 18)\n",
      "‚úì Removed 2 duplicates based on (id, year).\n"
     ]
    }
   ],
   "source": [
    "print(f\". Initial dataset shape before collapsing: {df.shape}\")\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates(subset=['id', 'year']).copy() # id <-> year\n",
    "after = df.shape[0]\n",
    "print(f\"‚úì Removed {before - after} duplicates based on (id, year).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fd2cd",
   "metadata": {},
   "source": [
    "#### 9. Filter minimum_nights to a reasonable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214bf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Filtering 'minimum_nights' to a reasonable range...\n",
      ". Initial dataset shape: (457204, 18)\n",
      "!  1 rows removed ‚Äî values outside [1, 2000].\n",
      "   -> Outlier range detected: min=100000000, max=100000000\n",
      "‚úì Dataset shape after filtering: (457203, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Filtering 'minimum_nights' to a reasonable range...\")\n",
    "print(f\". Initial dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify invalid entries before filtering\n",
    "invalid_mask = (df[\"minimum_nights\"] <= 0) | (df[\"minimum_nights\"] >= 2000)\n",
    "removed_count = invalid_mask.sum()\n",
    "\n",
    "if removed_count > 0:\n",
    "    removed_min = df.loc[invalid_mask, \"minimum_nights\"].min()\n",
    "    removed_max = df.loc[invalid_mask, \"minimum_nights\"].max()\n",
    "    print(f\"!  {removed_count:,} rows removed ‚Äî values outside [1, 2000].\")\n",
    "    print(f\"   -> Outlier range detected: min={removed_min}, max={removed_max}\")\n",
    "else:\n",
    "    print(\"‚úì No invalid 'minimum_nights' values found.\")\n",
    "\n",
    "# Apply filtering\n",
    "df = df[~invalid_mask].copy()\n",
    "\n",
    "print(f\"‚úì Dataset shape after filtering: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab2665",
   "metadata": {},
   "source": [
    "#### 10. Recompute host listing count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43385dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Recomputing listings per host/year...\n",
      "‚úì Recomputed host listing counts successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Recomputing listings per host/year...\")\n",
    "df['calculated_host_listings_count'] = (\n",
    "    df.groupby(['host_id', 'year'])['id'].transform('nunique')\n",
    ")\n",
    "print(\"‚úì Recomputed host listing counts successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca640245",
   "metadata": {},
   "source": [
    "#### 11. Dataset validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30946cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã  FINAL DATASET REPORT\n",
      "================================================================================\n",
      "\n",
      "üîπ [1] Dataset Structure Overview\n",
      "--------------------------------------------------------------------------------\n",
      "‚Ä¢ Total rows: 457,203\n",
      "‚Ä¢ Total columns: 18\n",
      "‚Ä¢ Columns available: id, name, host_id, host_name, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365, city, year, state\n",
      "\n",
      "üîπ [2] Missing Value Audit\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Missing values detected in the following columns:\n",
      "name                    44\n",
      "host_name               46\n",
      "last_review          96990\n",
      "reviews_per_month    96990\n",
      "dtype: int64\n",
      "\n",
      "üîπ [3] Statistical Distribution Overview\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä  PRICE  ‚Äî  457,203 observations\n",
      "   Mean: 238.94   |   Std: 822.68\n",
      "   Min: 0.00   |   Max: 100000.00\n",
      "   Skew: 58.59   |   Kurtosis: 5822.14\n",
      "   Thresholds of interest ‚Üí below 20 or above 10000\n",
      "   ‚Ä¢ 1,106 values (0.24%) fall beyond thresholds:\n",
      "     - 916 below 20  (min=0.00, max=19.00)\n",
      "     - 190 above 10000 (min=10286.00, max=100000.00)\n",
      "   ‚Ä¢ Distribution is right-skewed (long tail on high values) and leptokurtic ‚Äî sharply peaked with heavy tails.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä  MINIMUM_NIGHTS  ‚Äî  457,203 observations\n",
      "   Mean: 11.82   |   Std: 26.64\n",
      "   Min: 1.00   |   Max: 1250.00\n",
      "   Skew: 15.29   |   Kurtosis: 451.59\n",
      "   Thresholds of interest ‚Üí below 1 or above 100\n",
      "   ‚Ä¢ 2,450 values (0.54%) fall beyond thresholds:\n",
      "     - 2,450 above 100 (min=102.00, max=1250.00)\n",
      "   ‚Ä¢ Distribution is right-skewed (long tail on high values) and leptokurtic ‚Äî sharply peaked with heavy tails.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä  AVAILABILITY_365  ‚Äî  457,203 observations\n",
      "   Mean: 170.12   |   Std: 137.81\n",
      "   Min: 0.00   |   Max: 365.00\n",
      "   Skew: 0.11   |   Kurtosis: -1.55\n",
      "   Thresholds of interest ‚Üí below 0 or above 365\n",
      "   ‚Ä¢ All observations fall within defined thresholds.\n",
      "   ‚Ä¢ Distribution is approximately symmetric and platykurtic ‚Äî flatter, lighter tails.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä  REVIEWS_PER_MONTH  ‚Äî  360,213 observations\n",
      "   Mean: 1.54   |   Std: 1.80\n",
      "   Min: 0.01   |   Max: 101.42\n",
      "   Skew: 4.74   |   Kurtosis: 119.07\n",
      "   Thresholds of interest ‚Üí below 0 or above 30\n",
      "   ‚Ä¢ 38 values (0.01%) fall beyond thresholds:\n",
      "     - 38 above 30 (min=30.15, max=101.42)\n",
      "   ‚Ä¢ Distribution is right-skewed (long tail on high values) and leptokurtic ‚Äî sharply peaked with heavy tails.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîπ [4] Logical Consistency Checks\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Each (id, year) pair is unique ‚Äî no duplicate listings per year.\n",
      "\n",
      "üîπ [5] Host‚ÄìListing‚ÄìYear Consistency\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì 2020: Consistent ‚Äî 225,683 listings across 130,424 hosts.\n",
      "‚úì 2023: Consistent ‚Äî 231,520 listings across 119,582 hosts.\n",
      "\n",
      "üîπ [6] Categorical Column Audit\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì 'city' contains 28 unique values.\n",
      "‚úì 'state' contains 19 unique values.\n",
      "‚úì 'room_type' contains 4 unique values.\n",
      "‚úì 'year' contains 2 unique values.\n",
      "‚úì 'neighbourhood' contains 1459 unique values.\n",
      "‚úì 'host_name' contains 42550 unique values.\n",
      "\n",
      "üîπ [7] Geographic Coordinate Validation\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì All coordinates fall within valid Earth ranges (¬±90¬∞, ¬±180¬∞).\n",
      "\n",
      "================================================================================\n",
      "üìã  VALIDATION SUMMARY\n",
      "================================================================================\n",
      "‚ö†Ô∏è  Validation completed with 1 potential issues detected.\n",
      "   Please review warnings above before export.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã  FINAL DATASET REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "invalid_count = 0\n",
    "\n",
    "# --- 1. BASIC STRUCTURE ---\n",
    "print(\"\\nüîπ [1] Dataset Structure Overview\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"‚Ä¢ Total rows: {df.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Total columns: {df.shape[1]}\")\n",
    "print(f\"‚Ä¢ Columns available: {', '.join(df.columns)}\")\n",
    "\n",
    "# --- 2. MISSING VALUES ---\n",
    "print(\"\\nüîπ [2] Missing Value Audit\")\n",
    "print(\"-\" * 80)\n",
    "missing = df.isna().sum()\n",
    "if missing.any():\n",
    "    invalid_count += 1\n",
    "    print(\"‚ö†Ô∏è  Missing values detected in the following columns:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úì No missing values found in any column.\")\n",
    "\n",
    "# --- 3. STATISTICAL DISTRIBUTION (Threshold-based Descriptive Report) ---\n",
    "print(\"\\nüîπ [3] Statistical Distribution Overview\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "thresholds = {\n",
    "    \"price\": {\"low\": 20, \"high\": 10000},\n",
    "    \"minimum_nights\": {\"low\": 1, \"high\": 100},\n",
    "    \"availability_365\": {\"low\": 0, \"high\": 365},\n",
    "    \"reviews_per_month\": {\"low\": 0, \"high\": 30},\n",
    "}\n",
    "\n",
    "for col, thr in thresholds.items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è  Column '{col}' not found in dataset.\")\n",
    "        invalid_count += 1\n",
    "        continue\n",
    "\n",
    "    series = df[col].dropna()\n",
    "    mean, std = series.mean(), series.std()\n",
    "    s, k = skew(series), kurtosis(series)\n",
    "    min_val, max_val = series.min(), series.max()\n",
    "    below = series[series < thr[\"low\"]]\n",
    "    above = series[series > thr[\"high\"]]\n",
    "    total_extreme = len(below) + len(above)\n",
    "    ratio = 100 * total_extreme / len(series)\n",
    "\n",
    "    print(f\"\\nüìä  {col.upper()}  ‚Äî  {len(series):,} observations\")\n",
    "    print(f\"   Mean: {mean:.2f}   |   Std: {std:.2f}\")\n",
    "    print(f\"   Min: {min_val:.2f}   |   Max: {max_val:.2f}\")\n",
    "    print(f\"   Skew: {s:.2f}   |   Kurtosis: {k:.2f}\")\n",
    "    print(f\"   Thresholds of interest ‚Üí below {thr['low']} or above {thr['high']}\")\n",
    "\n",
    "    if total_extreme > 0:\n",
    "        print(f\"   ‚Ä¢ {total_extreme:,} values ({ratio:.2f}%) fall beyond thresholds:\")\n",
    "        if len(below):\n",
    "            print(f\"     - {len(below):,} below {thr['low']}  (min={below.min():.2f}, max={below.max():.2f})\")\n",
    "        if len(above):\n",
    "            print(f\"     - {len(above):,} above {thr['high']} (min={above.min():.2f}, max={above.max():.2f})\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ All observations fall within defined thresholds.\")\n",
    "\n",
    "    # Shape commentary\n",
    "    shape = \"approximately symmetric\"\n",
    "    if s > 0.5:\n",
    "        shape = \"right-skewed (long tail on high values)\"\n",
    "    elif s < -0.5:\n",
    "        shape = \"left-skewed (long tail on low values)\"\n",
    "\n",
    "    if k > 3:\n",
    "        tail = \"leptokurtic ‚Äî sharply peaked with heavy tails\"\n",
    "    elif k < 3:\n",
    "        tail = \"platykurtic ‚Äî flatter, lighter tails\"\n",
    "    else:\n",
    "        tail = \"mesokurtic ‚Äî similar to a normal curve\"\n",
    "\n",
    "    print(f\"   ‚Ä¢ Distribution is {shape} and {tail}.\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# --- 4. LOGICAL UNIQUENESS ---\n",
    "print(\"\\nüîπ [4] Logical Consistency Checks\")\n",
    "print(\"-\" * 80)\n",
    "dup_id_year = df.duplicated(subset=[\"id\", \"year\"]).sum()\n",
    "if dup_id_year:\n",
    "    invalid_count += 1\n",
    "    print(f\"‚ö†Ô∏è  {dup_id_year} duplicate (id, year) pairs found.\")\n",
    "else:\n",
    "    print(\"‚úì Each (id, year) pair is unique ‚Äî no duplicate listings per year.\")\n",
    "\n",
    "# --- 5. HOST‚ÄìLISTING CONSISTENCY ---\n",
    "print(\"\\nüîπ [5] Host‚ÄìListing‚ÄìYear Consistency\")\n",
    "print(\"-\" * 80)\n",
    "for year, group in df.groupby(\"year\"):\n",
    "    total_listings = group[\"id\"].nunique()\n",
    "    total_from_hosts = group.groupby(\"host_id\")[\"calculated_host_listings_count\"].first().sum()\n",
    "    if total_listings == total_from_hosts:\n",
    "        print(f\"‚úì {year}: Consistent ‚Äî {total_listings:,} listings across {group['host_id'].nunique():,} hosts.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {year}: Mismatch ‚Äî listings={total_listings}, summed host counts={total_from_hosts}\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# --- 6. CATEGORICAL SANITY ---\n",
    "print(\"\\nüîπ [6] Categorical Column Audit\")\n",
    "print(\"-\" * 80)\n",
    "categorical_cols = [\"city\", \"state\", \"room_type\", \"year\", \"neighbourhood\", \"host_name\"]\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        print(f\"‚úì '{col}' contains {unique_vals} unique values.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Column '{col}' missing from dataset.\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# --- 7. GEOGRAPHIC VALIDATION ---\n",
    "print(\"\\nüîπ [7] Geographic Coordinate Validation\")\n",
    "print(\"-\" * 80)\n",
    "if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "    invalid_lat = (~df[\"latitude\"].between(-90, 90)).sum()\n",
    "    invalid_lon = (~df[\"longitude\"].between(-180, 180)).sum()\n",
    "    if invalid_lat or invalid_lon:\n",
    "        invalid_count += 1\n",
    "        print(f\"‚ö†Ô∏è  {invalid_lat} invalid latitudes and {invalid_lon} invalid longitudes found.\")\n",
    "    else:\n",
    "        print(\"‚úì All coordinates fall within valid Earth ranges (¬±90¬∞, ¬±180¬∞).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Latitude/Longitude columns missing.\")\n",
    "    invalid_count += 1\n",
    "\n",
    "# --- 8. FINAL SUMMARY ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã  VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "if invalid_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Validation completed with {invalid_count} potential issues detected.\")\n",
    "    print(\"   Please review warnings above before export.\")\n",
    "else:\n",
    "    print(\"‚úÖ  Validation passed ‚Äî dataset is clean, consistent, and ready for export.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a8c5a",
   "metadata": {},
   "source": [
    "#### 12. Translate ZIP codes to neighbourhood names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8f5cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö°Ô∏é Translating ZIP codes to neighbourhood names...\n",
      "   Processing neighbourhood values...\n",
      "‚úì Neighbourhood translation completed.\n",
      "   Original unique values: 1,459\n",
      "   After translation: 1,411\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö°Ô∏é Translating ZIP codes to neighbourhood names...\")\n",
    "\n",
    "# Install pgeocode if needed: pip install pgeocode\n",
    "import pgeocode\n",
    "\n",
    "# Initialize US geocoder\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "def translate_neighbourhood(value):\n",
    "    \"\"\"\n",
    "    If value is numeric (ZIP code), translate to place name.\n",
    "    Otherwise, keep as is.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    \n",
    "    # Convert to string and check if it's numeric\n",
    "    str_value = str(value).strip()\n",
    "    \n",
    "    # Check if it looks like a ZIP code (5 digits)\n",
    "    if str_value.isdigit() and len(str_value) == 5:\n",
    "        try:\n",
    "            # Query the ZIP code\n",
    "            result = nomi.query_postal_code(str_value)\n",
    "            \n",
    "            # Check if we got a valid result\n",
    "            if not pd.isna(result.place_name):\n",
    "                # Return the place name (city/town name)\n",
    "                return result.place_name\n",
    "            else:\n",
    "                # If no result found, keep the ZIP code\n",
    "                return str_value\n",
    "        except Exception as e:\n",
    "            # On any error, keep original value\n",
    "            print(f\"   Warning: Could not translate ZIP {str_value}: {e}\")\n",
    "            return str_value\n",
    "    else:\n",
    "        # Not a ZIP code, keep as is\n",
    "        return str_value\n",
    "\n",
    "# Apply translation\n",
    "print(\"   Processing neighbourhood values...\")\n",
    "original_count = df['neighbourhood'].nunique()\n",
    "df['neighbourhood'] = df['neighbourhood'].apply(translate_neighbourhood)\n",
    "new_count = df['neighbourhood'].nunique()\n",
    "\n",
    "print(f\"‚úì Neighbourhood translation completed.\")\n",
    "print(f\"   Original unique values: {original_count:,}\")\n",
    "print(f\"   After translation: {new_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a07d",
   "metadata": {},
   "source": [
    "#### 13. Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2c811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Old file removed: out/dataset.csv\n",
      ". Exporting cleaned dataset to: out/dataset.csv\n",
      "‚úì CSV export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"out\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"dataset.csv\")\n",
    "\n",
    "# Explicitly remove old file if it exists\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\". Old file removed: {output_path}\")\n",
    "\n",
    "print(f\". Exporting cleaned dataset to: {output_path}\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úì CSV export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7b5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-preparation (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
