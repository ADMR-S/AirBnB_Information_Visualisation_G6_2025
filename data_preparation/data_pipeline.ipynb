{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d32c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70e53c",
   "metadata": {},
   "source": [
    "#### 1. Start: download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"kritikseth/us-airbnb-open-data\")\n",
    "print(f\"‚úì Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe26cf",
   "metadata": {},
   "source": [
    "#### 2. Start: listing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Scanning dataset folder for CSV files...\")\n",
    "csv_files = [f for f in os.listdir(path) if f.lower().endswith(\".csv\")]\n",
    "print(f\"‚úì CSV files found: {csv_files}\")\n",
    "\n",
    "dataframes = {}\n",
    "for csv in csv_files:\n",
    "    print(\"\\n\")\n",
    "    print(f\"‚ö°Ô∏é Loading {csv} ...\")\n",
    "    csv_path = os.path.join(path, csv)\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    dataframes[csv] = df\n",
    "    print(f\"‚úì {csv} loaded ‚Üí shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80b49b",
   "metadata": {},
   "source": [
    "#### 3. Start: check for 2020 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Checking for specific datasets (2020 & 2023)...\")\n",
    "df_2020 = dataframes.get(\"AB_US_2020.csv\")\n",
    "df_2023 = dataframes.get(\"AB_US_2023.csv\")\n",
    "print(f\"‚úì Found 2020 dataset: {df_2020 is not None}\")\n",
    "print(f\"‚úì Found 2023 dataset: {df_2023 is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3c329",
   "metadata": {},
   "source": [
    "#### 4. Clean up: drop unwanted columns if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e68597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Cleaning up datasets...\")\n",
    "print(f\". Initial 2020 dataset shape: {df_2020.shape}\")\n",
    "print(f\". Initial 2023 dataset shape: {df_2023.shape}\")\n",
    "\n",
    "to_drop = [\"neighbourhood_group\", \"number_of_reviews_ltm\"]\n",
    "\n",
    "for col in to_drop:\n",
    "    if col in df_2020.columns:\n",
    "        df_2020 = df_2020.drop(columns=col)\n",
    "    if col in df_2023.columns:\n",
    "        df_2023 = df_2023.drop(columns=col) \n",
    "\n",
    "\n",
    "\n",
    "# Make sure the columns match\n",
    "assert list(df_2020.columns) == list(df_2023.columns), (\n",
    "    \"Columns are not the same after cleanup!\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì 2020 dataset shape after cleanup: {df_2020.shape}\")\n",
    "print(f\"‚úì 2023 dataset shape after cleanup: {df_2023.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b14882",
   "metadata": {},
   "source": [
    "#### 5. Add 'year' column and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020[\"year\"] = \"2020\"\n",
    "df_2023[\"year\"] = \"2023\"\n",
    "\n",
    "df = pd.concat([df_2020, df_2023], ignore_index=True)\n",
    "\n",
    "print(f\"‚úì Merged dataset shape: {df.shape}\")\n",
    "print(f\"‚úì Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c4d8a",
   "metadata": {},
   "source": [
    "#### 6. Map each city to its US state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db117651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Correcting city typos...\")\n",
    "df[\"city\"] = df[\"city\"].replace({\n",
    "    \"San Clara Country\": \"Santa Clara County\"\n",
    "})\n",
    "\n",
    "city_to_state = {\n",
    "    \"New York City\": \"NY\",\n",
    "    \"Los Angeles\": \"CA\",\n",
    "    \"Broward County\": \"FL\",\n",
    "    \"San Diego\": \"CA\",\n",
    "    \"Austin\": \"TX\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Clark County\": \"NV\",\n",
    "    \"Nashville\": \"TN\",\n",
    "    \"Chicago\": \"IL\",\n",
    "    \"San Francisco\": \"CA\",\n",
    "    \"Washington D.C.\": \"DC\",\n",
    "    \"New Orleans\": \"LA\",\n",
    "    \"Seattle\": \"WA\",\n",
    "    \"Twin Cities MSA\": \"MN\",\n",
    "    \"Denver\": \"CO\",\n",
    "    \"Portland\": \"OR\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"Boston\": \"MA\",\n",
    "    \"Santa Clara County\": \"CA\",\n",
    "    \"San Mateo County\": \"CA\",\n",
    "    \"Oakland\": \"CA\",\n",
    "    \"Asheville\": \"NC\",\n",
    "    \"Jersey City\": \"NJ\",\n",
    "    \"Columbus\": \"OH\",\n",
    "    \"Santa Cruz County\": \"CA\",\n",
    "    \"Cambridge\": \"MA\",\n",
    "    \"Salem\": \"MA\",\n",
    "    \"Pacific Grove\": \"CA\"\n",
    "}\n",
    "\n",
    "print(\"‚ö°Ô∏é Mapping cities to states...\")\n",
    "df[\"state\"] = df[\"city\"].map(city_to_state)\n",
    "print(f\"‚úì 'state' column added with {df['state'].nunique()} unique values.\")\n",
    "\n",
    "# Check if any city didn't get mapped\n",
    "missing = df[df[\"state\"].isna()][\"city\"].unique()\n",
    "if len(missing):\n",
    "    print(f\"! Some cities are missing state mappings: {missing}\")\n",
    "    raise ValueError(\"Some cities are missing state mappings!\" + str(missing))\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73edc13",
   "metadata": {},
   "source": [
    "#### 7. Remove semantic duplicates (same host/listing details but different id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330569c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Removing semantic duplicates (same host/listing details but different id)...\")\n",
    "before = df.shape\n",
    "df = df.drop_duplicates(\n",
    "    subset=[\"host_id\", \"name\", \"latitude\", \"longitude\", \"room_type\", \"price\",\n",
    "            \"minimum_nights\", \"availability_365\", \"city\", \"year\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "print(f\"‚úì Removed {before[0] - df.shape[0]} duplicates. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccb2b0",
   "metadata": {},
   "source": [
    "#### 8. Remove duplicate of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\". Initial dataset shape before collapsing: {df.shape}\")\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates(subset=['id', 'year']).copy() # id <-> year\n",
    "after = df.shape[0]\n",
    "print(f\"‚úì Removed {before - after} duplicates based on (id, year).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fd2cd",
   "metadata": {},
   "source": [
    "#### 9. Filter minimum_nights to a reasonable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Filtering 'minimum_nights' to a reasonable range...\")\n",
    "print(f\". Initial dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify invalid entries before filtering\n",
    "invalid_mask = (df[\"minimum_nights\"] <= 0) | (df[\"minimum_nights\"] >= 2000)\n",
    "removed_count = invalid_mask.sum()\n",
    "\n",
    "if removed_count > 0:\n",
    "    removed_min = df.loc[invalid_mask, \"minimum_nights\"].min()\n",
    "    removed_max = df.loc[invalid_mask, \"minimum_nights\"].max()\n",
    "    print(f\"!  {removed_count:,} rows removed ‚Äî values outside [1, 2000].\")\n",
    "    print(f\"   -> Outlier range detected: min={removed_min}, max={removed_max}\")\n",
    "else:\n",
    "    print(\"‚úì No invalid 'minimum_nights' values found.\")\n",
    "\n",
    "# Apply filtering\n",
    "df = df[~invalid_mask].copy()\n",
    "\n",
    "print(f\"‚úì Dataset shape after filtering: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab2665",
   "metadata": {},
   "source": [
    "#### 10. Recompute host listing count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43385dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö°Ô∏é Recomputing listings per host/year...\")\n",
    "df['calculated_host_listings_count'] = (\n",
    "    df.groupby(['host_id', 'year'])['id'].transform('nunique')\n",
    ")\n",
    "print(\"‚úì Recomputed host listing counts successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca640245",
   "metadata": {},
   "source": [
    "#### 11. Dataset validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30946cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã  FINAL DATASET REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "invalid_count = 0\n",
    "\n",
    "# --- 1. BASIC STRUCTURE ---\n",
    "print(\"\\nüîπ [1] Dataset Structure Overview\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"‚Ä¢ Total rows: {df.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Total columns: {df.shape[1]}\")\n",
    "print(f\"‚Ä¢ Columns available: {', '.join(df.columns)}\")\n",
    "\n",
    "# --- 2. MISSING VALUES ---\n",
    "print(\"\\nüîπ [2] Missing Value Audit\")\n",
    "print(\"-\" * 80)\n",
    "missing = df.isna().sum()\n",
    "if missing.any():\n",
    "    invalid_count += 1\n",
    "    print(\"‚ö†Ô∏è  Missing values detected in the following columns:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úì No missing values found in any column.\")\n",
    "\n",
    "# --- 3. STATISTICAL DISTRIBUTION (Threshold-based Descriptive Report) ---\n",
    "print(\"\\nüîπ [3] Statistical Distribution Overview\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "thresholds = {\n",
    "    \"price\": {\"low\": 20, \"high\": 10000},\n",
    "    \"minimum_nights\": {\"low\": 1, \"high\": 100},\n",
    "    \"availability_365\": {\"low\": 0, \"high\": 365},\n",
    "    \"reviews_per_month\": {\"low\": 0, \"high\": 30},\n",
    "}\n",
    "\n",
    "for col, thr in thresholds.items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è  Column '{col}' not found in dataset.\")\n",
    "        invalid_count += 1\n",
    "        continue\n",
    "\n",
    "    series = df[col].dropna()\n",
    "    mean, std = series.mean(), series.std()\n",
    "    s, k = skew(series), kurtosis(series)\n",
    "    min_val, max_val = series.min(), series.max()\n",
    "    below = series[series < thr[\"low\"]]\n",
    "    above = series[series > thr[\"high\"]]\n",
    "    total_extreme = len(below) + len(above)\n",
    "    ratio = 100 * total_extreme / len(series)\n",
    "\n",
    "    print(f\"\\nüìä  {col.upper()}  ‚Äî  {len(series):,} observations\")\n",
    "    print(f\"   Mean: {mean:.2f}   |   Std: {std:.2f}\")\n",
    "    print(f\"   Min: {min_val:.2f}   |   Max: {max_val:.2f}\")\n",
    "    print(f\"   Skew: {s:.2f}   |   Kurtosis: {k:.2f}\")\n",
    "    print(f\"   Thresholds of interest ‚Üí below {thr['low']} or above {thr['high']}\")\n",
    "\n",
    "    if total_extreme > 0:\n",
    "        print(f\"   ‚Ä¢ {total_extreme:,} values ({ratio:.2f}%) fall beyond thresholds:\")\n",
    "        if len(below):\n",
    "            print(f\"     - {len(below):,} below {thr['low']}  (min={below.min():.2f}, max={below.max():.2f})\")\n",
    "        if len(above):\n",
    "            print(f\"     - {len(above):,} above {thr['high']} (min={above.min():.2f}, max={above.max():.2f})\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ All observations fall within defined thresholds.\")\n",
    "\n",
    "    # Shape commentary\n",
    "    shape = \"approximately symmetric\"\n",
    "    if s > 0.5:\n",
    "        shape = \"right-skewed (long tail on high values)\"\n",
    "    elif s < -0.5:\n",
    "        shape = \"left-skewed (long tail on low values)\"\n",
    "\n",
    "    if k > 3:\n",
    "        tail = \"leptokurtic ‚Äî sharply peaked with heavy tails\"\n",
    "    elif k < 3:\n",
    "        tail = \"platykurtic ‚Äî flatter, lighter tails\"\n",
    "    else:\n",
    "        tail = \"mesokurtic ‚Äî similar to a normal curve\"\n",
    "\n",
    "    print(f\"   ‚Ä¢ Distribution is {shape} and {tail}.\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# --- 4. LOGICAL UNIQUENESS ---\n",
    "print(\"\\nüîπ [4] Logical Consistency Checks\")\n",
    "print(\"-\" * 80)\n",
    "dup_id_year = df.duplicated(subset=[\"id\", \"year\"]).sum()\n",
    "if dup_id_year:\n",
    "    invalid_count += 1\n",
    "    print(f\"‚ö†Ô∏è  {dup_id_year} duplicate (id, year) pairs found.\")\n",
    "else:\n",
    "    print(\"‚úì Each (id, year) pair is unique ‚Äî no duplicate listings per year.\")\n",
    "\n",
    "# --- 5. HOST‚ÄìLISTING CONSISTENCY ---\n",
    "print(\"\\nüîπ [5] Host‚ÄìListing‚ÄìYear Consistency\")\n",
    "print(\"-\" * 80)\n",
    "for year, group in df.groupby(\"year\"):\n",
    "    total_listings = group[\"id\"].nunique()\n",
    "    total_from_hosts = group.groupby(\"host_id\")[\"calculated_host_listings_count\"].first().sum()\n",
    "    if total_listings == total_from_hosts:\n",
    "        print(f\"‚úì {year}: Consistent ‚Äî {total_listings:,} listings across {group['host_id'].nunique():,} hosts.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {year}: Mismatch ‚Äî listings={total_listings}, summed host counts={total_from_hosts}\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# --- 6. CATEGORICAL SANITY ---\n",
    "print(\"\\nüîπ [6] Categorical Column Audit\")\n",
    "print(\"-\" * 80)\n",
    "categorical_cols = [\"city\", \"state\", \"room_type\", \"year\", \"neighbourhood\", \"host_name\"]\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        print(f\"‚úì '{col}' contains {unique_vals} unique values.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Column '{col}' missing from dataset.\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# --- 7. GEOGRAPHIC VALIDATION ---\n",
    "print(\"\\nüîπ [7] Geographic Coordinate Validation\")\n",
    "print(\"-\" * 80)\n",
    "if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "    invalid_lat = (~df[\"latitude\"].between(-90, 90)).sum()\n",
    "    invalid_lon = (~df[\"longitude\"].between(-180, 180)).sum()\n",
    "    if invalid_lat or invalid_lon:\n",
    "        invalid_count += 1\n",
    "        print(f\"‚ö†Ô∏è  {invalid_lat} invalid latitudes and {invalid_lon} invalid longitudes found.\")\n",
    "    else:\n",
    "        print(\"‚úì All coordinates fall within valid Earth ranges (¬±90¬∞, ¬±180¬∞).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Latitude/Longitude columns missing.\")\n",
    "    invalid_count += 1\n",
    "\n",
    "# --- 8. FINAL SUMMARY ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã  VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "if invalid_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Validation completed with {invalid_count} potential issues detected.\")\n",
    "    print(\"   Please review warnings above before export.\")\n",
    "else:\n",
    "    print(\"‚úÖ  Validation passed ‚Äî dataset is clean, consistent, and ready for export.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a07d",
   "metadata": {},
   "source": [
    "#### 12. Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"out\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"dataset.csv\")\n",
    "\n",
    "# Explicitly remove old file if it exists\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\". Old file removed: {output_path}\")\n",
    "\n",
    "print(f\". Exporting cleaned dataset to: {output_path}\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úì CSV export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7b5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-preparation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
