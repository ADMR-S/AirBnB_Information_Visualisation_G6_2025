{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d32c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70e53c",
   "metadata": {},
   "source": [
    "#### 1. Start: download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚡︎ Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"kritikseth/us-airbnb-open-data\")\n",
    "print(f\"✓ Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe26cf",
   "metadata": {},
   "source": [
    "#### 2. Start: listing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Scanning dataset folder for CSV files...\")\n",
    "csv_files = [f for f in os.listdir(path) if f.lower().endswith(\".csv\")]\n",
    "print(f\"✓ CSV files found: {csv_files}\")\n",
    "\n",
    "dataframes = {}\n",
    "for csv in csv_files:\n",
    "    print(f\"\\n ⚡︎ Loading {csv} ...\")\n",
    "    csv_path = os.path.join(path, csv)\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    dataframes[csv] = df\n",
    "    print(f\"✓ {csv} loaded → shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80b49b",
   "metadata": {},
   "source": [
    "#### 3. Start: check for 2020 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Checking for specific datasets (2020 & 2023)...\")\n",
    "df_2020 = dataframes.get(\"AB_US_2020.csv\")\n",
    "df_2023 = dataframes.get(\"AB_US_2023.csv\")\n",
    "print(f\"✓ Found 2020 dataset: {df_2020 is not None}\")\n",
    "print(f\"✓ Found 2023 dataset: {df_2023 is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3c329",
   "metadata": {},
   "source": [
    "#### 4. Clean up: drop unwanted columns if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e68597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Cleaning up datasets...\")\n",
    "print(f\". Initial 2020 dataset shape: {df_2020.shape}\")\n",
    "print(f\". Initial 2023 dataset shape: {df_2023.shape}\")\n",
    "\n",
    "to_drop = [\"neighbourhood_group\", \"number_of_reviews_ltm\"]\n",
    "\n",
    "for col in to_drop:\n",
    "    if col in df_2020.columns:\n",
    "        df_2020 = df_2020.drop(columns=col)\n",
    "    if col in df_2023.columns:\n",
    "        df_2023 = df_2023.drop(columns=col) \n",
    "\n",
    "\n",
    "\n",
    "# Make sure the columns match\n",
    "assert list(df_2020.columns) == list(df_2023.columns), (\n",
    "    \"Columns are not the same after cleanup!\"\n",
    ")\n",
    "\n",
    "print(f\"✓ 2020 dataset shape after cleanup: {df_2020.shape}\")\n",
    "print(f\"✓ 2023 dataset shape after cleanup: {df_2023.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b14882",
   "metadata": {},
   "source": [
    "#### 5. Add 'year' column and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020[\"year\"] = \"2020\"\n",
    "df_2023[\"year\"] = \"2023\"\n",
    "\n",
    "df_merged = pd.concat([df_2020, df_2023], ignore_index=True)\n",
    "\n",
    "print(f\"✓ Merged dataset shape: {df_merged.shape}\")\n",
    "print(f\"✓ Columns: {df_merged.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c4d8a",
   "metadata": {},
   "source": [
    "#### 6. Map each city to its US state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db117651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Mapping cities to states...\")\n",
    "\n",
    "city_to_state = {\n",
    "    \"New York City\": \"NY\",\n",
    "    \"Los Angeles\": \"CA\",\n",
    "    \"Broward County\": \"FL\",\n",
    "    \"San Diego\": \"CA\",\n",
    "    \"Austin\": \"TX\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Clark County\": \"NV\",\n",
    "    \"Nashville\": \"TN\",\n",
    "    \"Chicago\": \"IL\",\n",
    "    \"San Francisco\": \"CA\",\n",
    "    \"Washington D.C.\": \"DC\",\n",
    "    \"New Orleans\": \"LA\",\n",
    "    \"Seattle\": \"WA\",\n",
    "    \"Twin Cities MSA\": \"MN\",\n",
    "    \"Denver\": \"CO\",\n",
    "    \"Portland\": \"OR\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"Boston\": \"MA\",\n",
    "    \"San Clara Country\": \"CA\",\n",
    "    \"Santa Clara County\": \"CA\",\n",
    "    \"San Mateo County\": \"CA\",\n",
    "    \"Oakland\": \"CA\",\n",
    "    \"Asheville\": \"NC\",\n",
    "    \"Jersey City\": \"NJ\",\n",
    "    \"Columbus\": \"OH\",\n",
    "    \"Santa Cruz County\": \"CA\",\n",
    "    \"Cambridge\": \"MA\",\n",
    "    \"Salem\": \"MA\",\n",
    "    \"Pacific Grove\": \"CA\"\n",
    "}\n",
    "\n",
    "df_merged[\"state\"] = df_merged[\"city\"].map(city_to_state)\n",
    "print(f\"✓ 'state' column added with {df_merged['state'].nunique()} unique values.\")\n",
    "\n",
    "# Check if any city didn't get mapped\n",
    "missing = df_merged[df_merged[\"state\"].isna()][\"city\"].unique()\n",
    "if len(missing):\n",
    "    print(f\"! Some cities are missing state mappings: {missing}\")\n",
    "    raise ValueError(\"Some cities are missing state mappings!\" + str(missing))\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73edc13",
   "metadata": {},
   "source": [
    "#### 7. Remove semantic duplicates (same host/listing details but different id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330569c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Removing semantic duplicates (same host/listing details but different id)...\")\n",
    "before = df_merged.shape\n",
    "df_merged = df_merged.drop_duplicates(\n",
    "    subset=[\"host_id\", \"name\", \"latitude\", \"longitude\", \"room_type\", \"price\",\n",
    "            \"minimum_nights\", \"availability_365\", \"city\", \"year\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "print(f\"✓ Removed {before[0] - df_merged.shape[0]} duplicates. New shape: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccb2b0",
   "metadata": {},
   "source": [
    "#### 8. Remove duplicate of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\". Initial dataset shape before collapsing: {df_merged.shape}\")\n",
    "df_unique = df_merged.drop_duplicates(subset=['id', 'year']).copy()\n",
    "print(f\"✓ Shape after collapsing to unique (id, year): {df_unique.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fd2cd",
   "metadata": {},
   "source": [
    "#### 9. Filter minimum_nights to a reasonable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Filtering minimum_nights to a reasonable range...\")\n",
    "print(f\". Initial dataset shape before filtering: {df_merged.shape}\")\n",
    "df_merged = df_merged[(df_merged[\"minimum_nights\"] > 0) & (df_merged[\"minimum_nights\"] < 2000)]\n",
    "print(f\"✓ Dataset shape after filtering: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab2665",
   "metadata": {},
   "source": [
    "#### 10. Recompute host listing count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43385dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ⚡︎ Recomputing listings per host/year...\")\n",
    "df_unique['calculated_host_listings_count'] = (\n",
    "    df_unique.groupby(['host_id', 'year'])['id'].transform('nunique')\n",
    ")\n",
    "print(\"✓ Recomputed host listing counts successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a07d",
   "metadata": {},
   "source": [
    "#### 11. Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"out\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"dataset.csv\")\n",
    "\n",
    "print(f\". Exporting cleaned dataset to: {output_path}\")\n",
    "df_unique.to_csv(output_path, index=False)\n",
    "print(\"✓ CSV export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7b5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-preparation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
